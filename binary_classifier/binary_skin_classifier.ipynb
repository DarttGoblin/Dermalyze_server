{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OwDhltbAqKoI"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import EfficientNetV2B0\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "L4NkfsJUqMWO"
      },
      "outputs": [],
      "source": [
        "# Configs\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "DATA_DIR = '/content/drive/MyDrive/binary_data'  # contains 'normal' and 'abnormal'\n",
        "CSV_PATH = '/content/drive/MyDrive/binary_data/HAM10000_metadata_balanced.csv'\n",
        "EPOCHS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WCap7-9QqPfQ"
      },
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "df = pd.read_csv(CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gQ0FgSB7qUZh"
      },
      "outputs": [],
      "source": [
        "# Filter for images in binary_data dir\n",
        "all_filenames = []\n",
        "for label in ['normal', 'abnormal']:\n",
        "    label_dir = os.path.join(DATA_DIR, label)\n",
        "    files = [f for f in os.listdir(label_dir) if f.endswith('.jpg')]\n",
        "    all_filenames.extend(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "reRR4Dj6qWlZ"
      },
      "outputs": [],
      "source": [
        "# Filter dataframe\n",
        "df = df[df['image_id'].apply(lambda x: f'{x}.jpg' in all_filenames)].copy()\n",
        "\n",
        "# Label binary target\n",
        "df['binary_label'] = df['dx'].apply(lambda x: 1 if x in ['mel', 'bcc', 'akiec'] else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JKHpiYYHqYzZ"
      },
      "outputs": [],
      "source": [
        "# Encode metadata\n",
        "df['sex'] = LabelEncoder().fit_transform(df['sex'].fillna('unknown'))\n",
        "df['localization'] = OneHotEncoder(sparse_output=False).fit_transform(df['localization'].fillna('unknown').values.reshape(-1, 1)).tolist()\n",
        "df['age'] = df['age'].fillna(df['age'].mean()) / 100.0  # Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PDdjPn0LqbcX"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def load_data(df):\n",
        "    images, ages, sexes, locs, labels = [], [], [], [], []\n",
        "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        label = 'abnormal' if row['binary_label'] == 1 else 'normal'\n",
        "        path = os.path.join(DATA_DIR, label, row['image_id'] + '.jpg')\n",
        "        img = load_img(path, target_size=IMG_SIZE)\n",
        "        img_array = img_to_array(img) / 255.0\n",
        "        images.append(img_array)\n",
        "        ages.append(row['age'])\n",
        "        sexes.append(row['sex'])\n",
        "        locs.append(row['localization'])\n",
        "        labels.append(row['binary_label'])\n",
        "    return (\n",
        "        np.array(images),\n",
        "        np.array(ages).reshape(-1, 1),\n",
        "        np.array(sexes).reshape(-1, 1),\n",
        "        np.array(locs),\n",
        "        np.array(labels)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XM2EpI_GqeCt",
        "outputId": "32dddd2f-0319-4e3b-9663-3564dc9d682e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3909/3909 [18:14<00:00,  3.57it/s]\n"
          ]
        }
      ],
      "source": [
        "images, ages, sexes, locs, labels = load_data(df)\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_val, age_train, age_val, sex_train, sex_val, loc_train, loc_val, y_train, y_val = train_test_split(\n",
        "    images, ages, sexes, locs, labels, test_size=0.2, stratify=labels, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgwQl1Ciqiov",
        "outputId": "83e36d03-1de4-44a8-c7e1-389add983a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-b0_notop.h5\n",
            "\u001b[1m24274472/24274472\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Build model\n",
        "img_input = layers.Input(shape=IMG_SIZE + (3,))\n",
        "age_input = layers.Input(shape=(1,))\n",
        "sex_input = layers.Input(shape=(1,))\n",
        "loc_input = layers.Input(shape=(locs.shape[1],))\n",
        "\n",
        "base = EfficientNetV2B0(include_top=False, weights='imagenet', input_tensor=img_input)\n",
        "x = layers.GlobalAveragePooling2D()(base.output)\n",
        "\n",
        "meta = layers.Concatenate()([age_input, sex_input, loc_input])\n",
        "meta = layers.Dense(32, activation='relu')(meta)\n",
        "\n",
        "combined = layers.Concatenate()([x, meta])\n",
        "combined = layers.Dense(64, activation='relu')(combined)\n",
        "combined = layers.Dropout(0.5)(combined)\n",
        "output = layers.Dense(1, activation='sigmoid')(combined)\n",
        "\n",
        "model = Model(inputs=[img_input, age_input, sex_input, loc_input], outputs=output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lx4ez7rqqlV5"
      },
      "outputs": [],
      "source": [
        "# Compile\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZcv6E_Gqnr6",
        "outputId": "781c3a6b-ef52-4ac3-ec53-87347655b923"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 699ms/step - accuracy: 0.6585 - loss: 0.6086 - val_accuracy: 0.5000 - val_loss: 0.7873\n",
            "Epoch 2/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 128ms/step - accuracy: 0.7314 - loss: 0.5521 - val_accuracy: 0.5192 - val_loss: 0.8017\n",
            "Epoch 3/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 124ms/step - accuracy: 0.7437 - loss: 0.5047 - val_accuracy: 0.5000 - val_loss: 1.0809\n",
            "Epoch 4/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 125ms/step - accuracy: 0.7932 - loss: 0.4711 - val_accuracy: 0.5038 - val_loss: 0.8515\n",
            "Epoch 5/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 127ms/step - accuracy: 0.8039 - loss: 0.4393 - val_accuracy: 0.5627 - val_loss: 0.6712\n",
            "Epoch 6/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 130ms/step - accuracy: 0.8578 - loss: 0.3529 - val_accuracy: 0.5013 - val_loss: 1.1708\n",
            "Epoch 7/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 132ms/step - accuracy: 0.8393 - loss: 0.3735 - val_accuracy: 0.4987 - val_loss: 0.7014\n",
            "Epoch 8/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 129ms/step - accuracy: 0.8955 - loss: 0.2626 - val_accuracy: 0.5000 - val_loss: 2.3565\n",
            "Epoch 9/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 128ms/step - accuracy: 0.8961 - loss: 0.2488 - val_accuracy: 0.5064 - val_loss: 0.7259\n",
            "Epoch 10/10\n",
            "\u001b[1m98/98\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 126ms/step - accuracy: 0.9312 - loss: 0.1825 - val_accuracy: 0.4987 - val_loss: 1.1334\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e120562d90>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train\n",
        "model.fit(\n",
        "    [X_train, age_train, sex_train, loc_train], y_train,\n",
        "    validation_data=([X_val, age_val, sex_val, loc_val], y_val),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "evzuVVCHqpHz"
      },
      "outputs": [],
      "source": [
        "# Save model\n",
        "model.save('binary_skin_classifier.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75qZ4-qU1Xy2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
